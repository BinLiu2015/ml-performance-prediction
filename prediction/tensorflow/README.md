##### Background
When experimenting with benchmarks for fundamental operations (mostly GEMM/fully connected layers and Convolutions) on the one side and full CNNs on the other side it became clear that decomposing a CNN into its single ops and adding their computation times is a pretty good estimate for  the network computation time - This works for both, only forward pass and forward+backward pass. I can supply data/figures for this.
Predicting the computation time for single ops seems more challenging, it is not predictable from the mere number of floating point multiply/accumulate operations.

I have tried a data driven approach, initially to get a better idea what is going on, now I think this could even be a nice tool in itself.

##### Data generation
In "Generate_train_data" you'll find code to generate benchmarks for convolutional and fully connected layers with random parameters (for Convolutions: batchsize, matrix size, kernel size, number of input features, number of output features, whether or not to 0-pad the matrix, the type of optimizer in case of assessing backward pass; for fully connected: batchsize, number of input features, number of output features, the type of optimizer). I generate a class for the operation (in benchmark_conv.py/benchmark_dense.py) and a class for the execution (in run_benchmark.py). The benchmark is repeated a number of times and averaged. To ensure good training data quality this experiment is again repeated 5x - now we can for instance require low variance/difference of these 5 results. Then I take the median value as benchmark result. Results and used parameters are saved as .npy, and organised in a pandas dataframe as .pkl file.

##### Data prediction
I use these results to build a simple feed-forward model with relu activations and L2 regularisation for predicting compute times for convolutions and fully connected layers. Some data handling around this of it is in the notebook model_compTime.ipynb. Basically it combines different datasets - I've used multiple GPUs to generate training data, I have done single runs for forward pass only and forward/backward pass - plots some histograms and starts the training. Data preprocessing (normalising and generation of train/test/validation datasets) is in dataprep.py, the model class itself is defined in performance_prediction_model.py. The hyperparameters of this model don't really seem to matter very much (although the model requires a certain complexity, either depth or width). It is much, much better than a linear regression [number of floating point operations] --> [computation time], but still not 100% accurate, so there might be features I have not been thinking about (or there is just some randomness / parameters that we cannot control). Tests with 2 different GPUs (V100, 1080Ti; I used the peak performance as claimed by NVIDIA as only feature to characterize the GPU) worked well, but I guess the model just learned two different states rather than something that is generalisable over more GPU types. We would likely need data from additional different GPUs and more GPU features (memory, latency, bandwidth), probably also other system features (CPU, memory, ...) if we want to make it generally applicable.
